import os
import json
from cs336_alignment.common import ordered_filename
from tokenizers import Tokenizer
from transformers import PreTrainedModel
from einops import rearrange
from vllm import LLM, SamplingParams
from collections.abc import Callable
from cs336_alignment.vllm import EvalResult
import torch


def tokenize_prompt_and_output(
    prompt_strs: list[str],
    output_strs: list[str],
    tokenizer: Tokenizer,
    device: str | None = None,
) -> dict[str, torch.Tensor]:
    assert len(prompt_strs) == len(output_strs)

    prompt_tokens_list = []
    output_tokens_list = []
    concated_tokens_list = []

    for prompt_str, output_str in zip(prompt_strs, output_strs):
        prompt_tokens = tokenizer.encode(
            prompt_str,
        )
        output_tokens = tokenizer.encode(output_str)

        prompt_tokens_list.append(prompt_tokens)
        output_tokens_list.append(output_tokens)
        concated_tokens_list.append(prompt_tokens + output_tokens)

    max_len = max(len(t) for t in concated_tokens_list)

    input_ids = torch.full(
        (len(concated_tokens_list), max_len - 1), tokenizer.pad_token_id
    )
    labels = torch.full(
        (len(concated_tokens_list), max_len - 1), tokenizer.pad_token_id
    )
    response_mask = torch.zeros(
        (len(concated_tokens_list), max_len - 1), dtype=torch.bool
    )

    for i, tokens in enumerate(concated_tokens_list):
        n_tokens = min(len(tokens), max_len - 1)

        prompt_tokens = prompt_tokens_list[i]
        input_ids[i, :n_tokens] = torch.tensor(tokens[:n_tokens])
        labels[i, : len(tokens) - 1] = torch.tensor(tokens[1:])
        response_mask[i, len(prompt_tokens) - 1 : len(tokens) - 1] = True

    if device is not None:
        input_ids = input_ids.to(device)
        labels = labels.to(device)
        response_mask = response_mask.to(device)

    return {
        "input_ids": input_ids,
        "labels": labels,
        "response_mask": response_mask,
    }


def compute_entropy(logits: torch.Tensor) -> torch.Tensor:
    log_probs = torch.log_softmax(logits, dim=-1)
    probs = torch.exp(log_probs)
    return -torch.sum(probs * log_probs, dim=-1)


def get_response_log_probs(
    model: PreTrainedModel,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
    return_token_entropy: bool,
) -> dict[str, torch.Tensor]:
    logits = model(input_ids).logits

    all_log_probs = torch.log_softmax(logits, dim=-1)  # b t v
    labels_idx = rearrange(labels, "b t -> b t 1")

    log_probs_of_labels = torch.gather(all_log_probs, dim=-1, index=labels_idx)  # b t 1
    log_probs_of_labels = log_probs_of_labels.squeeze(dim=-1)  # b t

    result = {"log_probs": log_probs_of_labels}

    if return_token_entropy:
        result["token_entropy"] = compute_entropy(logits)

    return result


def masked_normalize(
    tensor: torch.Tensor,
    mask: torch.Tensor,
    normalize_constant: float,
    dim: int | None = None,
) -> torch.Tensor:
    masked = tensor * mask
    return masked.sum(dim) / normalize_constant


def log_generations(
    vllm_model: LLM,
    hf_model: PreTrainedModel,
    tokenizer: Tokenizer,
    prompts: list[str],
    ground_truths: list[str],
    reward_fn: Callable[[str, str], dict[str, float]] | None = None,
    sampling_params: SamplingParams | None = None,
    out_dir: str | None = "out",
    out_file: str | None = None,
) -> list[EvalResult]:
    """
    Prompts the model to generate responses for some given prompts (e.g., sampled from the validation set). Logs at least the following for each example:
    1. The input prompt.
    2. The response generated by the SFT/RL model.
    3. The ground-truth answer.
    4. The reward information, including format, answer, and total reward.
    5. The average token entropy of the response.
    6. The average response length, average response length for correct responses, and average response length for incorrect responses.
    7. The average token entropy for correct responses, and average token entropy for incorrect responses.
    """
    # Use default sampling params if not provided
    if sampling_params is None:
        sampling_params = SamplingParams(
            temperature=1.0,
            top_p=1.0,
            max_tokens=1024,
            stop=["</answer>"],
            include_stop_str_in_output=True,
        )

    outputs = vllm_model.generate(prompts, sampling_params)

    os.makedirs(out_dir, exist_ok=True)
    filename = out_file or f"{ordered_filename('log_generations')}.jsonl"
    outpath = os.path.join(out_dir, filename)

    results = []
    total_response_length = 0
    correct_response_lengths = []
    incorrect_response_lengths = []
    total_entropy = 0
    n_tokens_total = 0

    for i, output in enumerate(outputs):
        prompt = output.prompt
        completion = output.outputs[0].text
        ground_truth = ground_truths[i]

        rewards = {}
        if reward_fn is not None:
            rewards = reward_fn(completion, ground_truth)
            is_correct = rewards.get("reward", 0) > 0
        else:
            is_correct = False

        # Tokenize prompt and completion to compute entropy
        prompt_output_dict = tokenize_prompt_and_output(
            [prompt],
            [completion],
            tokenizer,
            device=hf_model.device if hasattr(hf_model, "device") else None,
        )

        input_ids = prompt_output_dict["input_ids"]
        labels = prompt_output_dict["labels"]
        response_mask = prompt_output_dict["response_mask"]

        # Get log probs and entropy from HF model
        with torch.no_grad():
            response_info = get_response_log_probs(
                hf_model, input_ids, labels, return_token_entropy=True
            )

        # Calculate average token entropy for the response
        token_entropy = response_info["token_entropy"]
        masked_entropy = token_entropy * response_mask
        total_entropy += masked_entropy.sum().item()

        n_response_tokens = response_mask.sum().item()
        response_length = n_response_tokens
        total_response_length += response_length

        if is_correct:
            correct_response_lengths.append(response_length)
        else:
            incorrect_response_lengths.append(response_length)

        n_tokens_total += n_response_tokens

        result = EvalResult(
            prompt=prompt,
            completion=completion,
            ground_truth=ground_truth,
            rewards=rewards,
        )

        results.append(result)

    # Calculate aggregate metrics
    n_examples = len(prompts)
    avg_response_length = total_response_length / n_examples if n_examples > 0 else 0
    avg_correct_length = (
        sum(correct_response_lengths) / len(correct_response_lengths)
        if correct_response_lengths
        else 0
    )
    avg_incorrect_length = (
        sum(incorrect_response_lengths) / len(incorrect_response_lengths)
        if incorrect_response_lengths
        else 0
    )
    avg_token_entropy = total_entropy / n_tokens_total if n_tokens_total > 0 else 0

    # Write results to file
    with open(outpath, "w") as f:
        # Write summary metrics
        summary = {
            "n_examples": n_examples,
            "avg_response_length": avg_response_length,
            "avg_correct_response_length": avg_correct_length,
            "avg_incorrect_response_length": avg_incorrect_length,
            "avg_token_entropy": avg_token_entropy,
        }
        f.write(json.dumps(summary) + "\n")

        # Write individual results
        for result in results:
            f.write(result.model_dump_json() + "\n")

    return results
